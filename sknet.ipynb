{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 64])\n",
      "torch.Size([8, 1, 64])\n",
      "torch.Size([8, 1, 64])\n",
      "out shape : torch.Size([8, 64, 32, 32])\n",
      "loss value : 0.3803827166557312\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SKConv(nn.Module):\n",
    "    def __init__(self, features, WH, M, G, r, stride=1 ,L=32):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            features: input channel dimensionality.\n",
    "            WH: input spatial dimensionality, used for GAP kernel size.\n",
    "            M: the number of branchs.\n",
    "            G: num of convolution groups.\n",
    "            r: the radio for compute d, the length of z.\n",
    "            stride: stride, default 1.\n",
    "            L: the minimum dim of the vector z in paper, default 32.\n",
    "        \"\"\"\n",
    "        super(SKConv, self).__init__()\n",
    "        d = max(int(features/r), L)\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "        self.convs = nn.ModuleList([])\n",
    "        for i in range(M):\n",
    "            self.convs.append(nn.Sequential(\n",
    "                nn.Conv2d(features, features, kernel_size=3+i*2, stride=stride, padding=1+i, groups=G),\n",
    "                nn.BatchNorm2d(features),\n",
    "                nn.ReLU(inplace=False)\n",
    "            ))\n",
    "        # self.gap = nn.AvgPool2d(int(WH/stride))\n",
    "        self.fc = nn.Linear(features, d)\n",
    "        self.fcs = nn.ModuleList([])\n",
    "        for i in range(M):\n",
    "            self.fcs.append(\n",
    "                nn.Linear(d, features)\n",
    "            )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            fea = conv(x).unsqueeze_(dim=1)\n",
    "            # print(fea.shape)\n",
    "            if i == 0:\n",
    "                feas = fea\n",
    "            else:\n",
    "                feas = torch.cat([feas, fea], dim=1)\n",
    "            # print(feas.shape)\n",
    "        fea_U = torch.sum(feas, dim=1)\n",
    "        # fea_s = self.gap(fea_U).squeeze_()\n",
    "        fea_s = fea_U.mean(-1).mean(-1)\n",
    "        fea_z = self.fc(fea_s)\n",
    "        for i, fc in enumerate(self.fcs):\n",
    "            vector = fc(fea_z).unsqueeze_(dim=1)\n",
    "            # print(vector.shape)\n",
    "            if i == 0:\n",
    "                attention_vectors = vector\n",
    "            else:\n",
    "                attention_vectors = torch.cat([attention_vectors, vector], dim=1)\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)\n",
    "        fea_v = (feas * attention_vectors).sum(dim=1)\n",
    "        return fea_v\n",
    "\n",
    "\n",
    "class SKUnit(nn.Module):\n",
    "    def __init__(self, in_features, out_features, WH, M, G, r, mid_features=None, stride=1, L=32):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            in_features: input channel dimensionality.\n",
    "            out_features: output channel dimensionality.\n",
    "            WH: input spatial dimensionality, used for GAP kernel size.\n",
    "            M: the number of branchs.\n",
    "            G: num of convolution groups.\n",
    "            r: the radio for compute d, the length of z.\n",
    "            mid_features: the channle dim of the middle conv with stride not 1, default out_features/2.\n",
    "            stride: stride.\n",
    "            L: the minimum dim of the vector z in paper.\n",
    "        \"\"\"\n",
    "        super(SKUnit, self).__init__()\n",
    "        if mid_features is None:\n",
    "            mid_features = int(out_features/2)\n",
    "        self.feas = nn.Sequential(\n",
    "            nn.Conv2d(in_features, mid_features, 1, stride=1),\n",
    "            nn.BatchNorm2d(mid_features),\n",
    "            SKConv(mid_features, WH, M, G, r, stride=stride, L=L),\n",
    "            nn.BatchNorm2d(mid_features),\n",
    "            nn.Conv2d(mid_features, out_features, 1, stride=1),\n",
    "            nn.BatchNorm2d(out_features)\n",
    "        )\n",
    "        if in_features == out_features: # when dim not change, in could be added diectly to out\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else: # when dim not change, in should also change dim to be added to out\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_features, out_features, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_features)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fea = self.feas(x)\n",
    "        return fea + self.shortcut(x)\n",
    "\n",
    "\n",
    "class SKNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super(SKNet, self).__init__()\n",
    "        self.basic_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        ) # 32x32\n",
    "        self.stage_1 = nn.Sequential(\n",
    "            SKUnit(64, 256, 32, 2, 8, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            SKUnit(256, 256, 32, 2, 8, 2),\n",
    "            nn.ReLU(),\n",
    "            SKUnit(256, 256, 32, 2, 8, 2),\n",
    "            nn.ReLU()\n",
    "        ) # 32x32\n",
    "        self.stage_2 = nn.Sequential(\n",
    "            SKUnit(256, 512, 32, 2, 8, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            SKUnit(512, 512, 32, 2, 8, 2),\n",
    "            nn.ReLU(),\n",
    "            SKUnit(512, 512, 32, 2, 8, 2),\n",
    "            nn.ReLU()\n",
    "        ) # 16x16\n",
    "        self.stage_3 = nn.Sequential(\n",
    "            SKUnit(512, 1024, 32, 2, 8, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            SKUnit(1024, 1024, 32, 2, 8, 2),\n",
    "            nn.ReLU(),\n",
    "            SKUnit(1024, 1024, 32, 2, 8, 2),\n",
    "            nn.ReLU()\n",
    "        ) # 8x8\n",
    "        self.pool = nn.AvgPool2d(8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, class_num),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea = self.basic_conv(x)\n",
    "        fea = self.stage_1(fea)\n",
    "        fea = self.stage_2(fea)\n",
    "        fea = self.stage_3(fea)\n",
    "        fea = self.pool(fea)\n",
    "        fea = torch.squeeze(fea)\n",
    "        fea = self.classifier(fea)\n",
    "        return fea\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    x = torch.rand(8, 64, 32, 32)\n",
    "    conv = SKConv(64, 32, 3, 8, 2)\n",
    "    out = conv(x)\n",
    "    criterion = nn.L1Loss()\n",
    "    loss = criterion(out, x)\n",
    "    loss.backward()\n",
    "    print('out shape : {}'.format(out.shape))\n",
    "    print('loss value : {}'.format(loss))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4d80541b0b515a3e45ee7a1ccc48716ea2070627a1ff6669ac05393be4e62eb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
